# %%
import torch
import numpy as np
from torch.autograd import Variable
from torch.utils.tensorboard import SummaryWriter


def create_vocab(rawtxt):
    letters = list(set(rawtxt))
    lettermap = dict(enumerate(letters))  # created the dictionary mapping

    return lettermap


class Tokeniser:
    def __init__(self, txt):

        unique_chars = set(txt)
        self.id_to_token = dict(enumerate(unique_chars))
        self.token_to_id = {v: k for k, v in self.id_to_token.items()}

    def encode(self, str):
        return [self.token_to_id[char] for char in str.strip().lower()]

    def decode(self, token_ids):
        return "".join([self.id_to_token[id] for id in token_ids])

# return a random batch for training


def random_chunk(chunk_size):
    k = np.random.randint(0, len(X)-chunk_size)
    return X[k:k+chunk_size], Y[k:k+chunk_size]


class RNN(torch.nn.Module):
    def __init__(self, vocab_size, hidden_size, n_layers=1):
        super().__init__()
        # store input parameters in the object so we can use them later on
        self.input_size = vocab_size
        self.hidden_size = hidden_size
        self.output_size = vocab_size
        self.n_layers = n_layers

        # required functions for model
        self.encoder = torch.nn.Embedding(vocab_size, hidden_size)
        self.rnn = torch.nn.RNN(hidden_size, hidden_size,
                                n_layers, batch_first=True)
        self.decoder = torch.nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden):
        # encode our input into a vector embedding
        x = self.encoder(x.view(1, -1))
        # calculate the output from our rnn based on our input and previous hidden state
        output, hidden = self.rnn(x.view(1, 1, -1), hidden)
        # calculate our output based on output of rnn
        output = self.decoder(output.view(1, -1))

        return output, hidden

    def init_hidden(self):
        # initialize our hidden state to a matrix of 0s
        return torch.zeros(self.n_layers, 1, self.hidden_size)


def train(model, epochs=1):
    writer = SummaryWriter()
    criterion = torch.nn.CrossEntropyLoss()  # define our cost function
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # choose optimizer
    n_steps = 0
    for epoch in range(epochs):
        epoch_cost = 0  # stored the cost per epoch
        generated = ''  # stores the text generated by our model each epoch

        # given our chunk size, how many chunks do we need to optimizer over to have gone thorough our whole dataset
        n_chunks = len(X)//chunk_size
        for chunk_idx in range(n_chunks):
            h = model.init_hidden()  # initialize our hidden state to 0s
            cost = 0  # cost for this chunk
            # get a random sequence chunk to train
            x, y = random_chunk(chunk_size)

            # sequentially input each character in our sequence and calculate loss
            for i in range(chunk_size):
                # calculate outputs based on input and previous hidden state
                out, h = model.forward(x[i], h)

                # based on our output, what character does our network predict is next?
                predicted_token_id = torch.argmax(out).item()
                letter = tokeniser.id_to_token[predicted_token_id]
                generated += letter  # add the predicted letter to our generated sequence

                # add the cost for this input to the cost for this current chunk
                target = y[i].unsqueeze(0)
                cost += criterion(out, target)

            # based on the sum of the cost for this sequence - backpropagate through time - calculating the gradients and updating our weights
            optimizer.zero_grad()
            cost.backward()
            optimizer.step()
            writer.add_scalar("Loss/Train", cost.item(), n_steps)
            n_steps += 1

            epoch_cost += cost  # add the cost of this sequence to the cost of this epoch
        # divide by the number of chunks per epoch to get average cost per epoch
        epoch_cost /= n_chunks

        print('Epoch ', epoch, ' Avg cost/chunk: ', epoch_cost.item())
        # print('Generated text: ', generated, '\n')
        writer.add_text("Generated Text", generated[:100], epoch)


if __name__ == "__main__":
    with open('lyrics.txt', 'r') as file:
        txt = file.read()
    txt = txt.lower()

    tokeniser = Tokeniser(txt)

    X = tokeniser.encode(txt)
    Y = np.roll(X, -1, axis=0)
    X = torch.tensor(X)
    Y = torch.tensor(Y)

    n_tokens = len(set(txt))

    # HYPER-PARAMS
    lr = 0.005
    epochs = 500
    chunk_size = 100  # the length of the sequences which we will optimize over

    # instantiate our model from the class defined earlier
    myrnn = RNN(n_tokens, 50, 2)
    train(myrnn, epochs)

# %%
